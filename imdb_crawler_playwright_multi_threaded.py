#!/usr/bin/env python
# -*- coding: UTF-8 -*-

"""
@Project    ÔºöIMDBCrawler
@File       Ôºöimdb_crawler_playwright_multi_threaded.py
@Author     ÔºöIronmanJay
@Date       Ôºö2025/7/4 23:49
@Describe   Ôºö‰ΩøÁî®PlaywrightÔºàÂ§öÁ∫øÁ®ãÔºâÂØπIMDBÁΩëÁ´ôÁöÑHTMLÈ°µÈù¢ËøõË°åÁà¨Âèñ
"""

import os
import time
import traceback
import random
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from playwright.sync_api import sync_playwright

# ==== ÈÖçÁΩÆÈ°π ====
ROOT_DIR = os.getcwd()
IMDB_ID_FILE = "data.txt"
OUTPUT_DIR = "debug_results"
FAILED_FILE = "failed_ids.txt"
TIMEOUT = 10000
RETRY_COUNT = 2
HEADLESS = True
MAX_WORKERS = 4

# ==== Â∑•ÂÖ∑ÂáΩÊï∞ ====
def read_imdb_ids_from_file(filename="data.txt"):
    filepath = os.path.join(ROOT_DIR, filename)
    imdb_ids = []
    try:
        with open(filepath, "r", encoding="utf-8") as file:
            for line in file:
                line = line.strip()
                if line.startswith("tt") and len(line) >= 9:
                    imdb_ids.append(line)
        return imdb_ids
    except Exception as e:
        print(f"ËØªÂèñIMDb IDÂ§±Ë¥•: {e}")
        return []

def remove_id_from_file(imdb_id, filename="data.txt"):
    filepath = os.path.join(ROOT_DIR, filename)
    try:
        with open(filepath, "r", encoding="utf-8") as file:
            lines = file.readlines()
        new_lines = [line for line in lines if line.strip() != imdb_id]
        with open(filepath, "w", encoding="utf-8") as file:
            file.writelines(new_lines)
    except Exception as e:
        print(f"ÁßªÈô§IDÂ§±Ë¥•: {imdb_id} - {e}")
        traceback.print_exc()

def save_html(page, imdb_id, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    path = os.path.join(output_dir, f"{imdb_id}.html")
    with open(path, "w", encoding="utf-8") as f:
        f.write(page.content())
    print(f"‚úÖ [{imdb_id}] Â∑≤‰øùÂ≠ò: {path}")

def is_challenge_page(html):
    return "awswaf" in html.lower() or "challenge-container" in html.lower()

def fetch_imdb_page(page, imdb_id):
    url = f"https://www.imdb.com/title/{imdb_id}/plotsummary/"
    page.set_extra_http_headers({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.imdb.com/"
    })

    for attempt in range(1, RETRY_COUNT + 1):
        try:
            page.goto(url, timeout=TIMEOUT)
            page.wait_for_selector("#summaries", timeout=5000)
            html = page.content()
            if is_challenge_page(html):
                page.reload(timeout=TIMEOUT)
                html = page.content()
                if is_challenge_page(html):
                    raise Exception("‰ªç‰∏∫ÊåëÊàòÈ°µ")
            return True
        except Exception as e:
            print(f"‚ùå [{imdb_id}] Á¨¨{attempt}Ê¨°Â§±Ë¥•: {e}")
            if attempt < RETRY_COUNT:
                wait = 2 + attempt * 2 + random.uniform(0.5, 1.5)
                print(f"‚è≥ Á≠âÂæÖ {wait:.1f}s ÂêéÈáçËØï...")
                time.sleep(wait)
    return False

# ==== Â§öÁ∫øÁ®ãÂÖ±‰∫´ContextÊâßË°åÈÄªËæë ====
def fetch_all_parallel(imdb_ids, output_dir):
    failed_ids = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=HEADLESS, args=["--no-sandbox", "--disable-dev-shm-usage"])
        context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")

        # Á¶ÅÁî®‰∏çÂøÖË¶ÅÁöÑËµÑÊ∫ê
        context.route("**/*", lambda route, request:
            route.abort() if request.resource_type in ["image", "font", "stylesheet"] else route.continue_()
        )

        lock = Lock()  # ÊéßÂà∂Êó•ÂøóÊàñÂÖ±‰∫´ËµÑÊ∫êËÆøÈóÆ

        def worker(index_imdbid):
            index, imdb_id = index_imdbid
            try:
                with lock:
                    print(f"üì• Ê≠£Âú®Â§ÑÁêÜ {index + 1}/{len(imdb_ids)}: {imdb_id}")
                page = context.new_page()
                success = fetch_imdb_page(page, imdb_id)
                if success:
                    save_html(page, imdb_id, output_dir)
                    remove_id_from_file(imdb_id)
                else:
                    failed_ids.append(imdb_id)
            except Exception as e:
                print(f"‚ùå ÂºÇÂ∏∏: {imdb_id} - {e}")
                failed_ids.append(imdb_id)
            finally:
                try:
                    page.close()
                except:
                    pass

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            executor.map(worker, enumerate(imdb_ids))

        context.close()
        browser.close()

    return failed_ids

# ==== ‰∏ªÂáΩÊï∞ ====
if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ IMDb Â§öÁ∫øÁ®ãÁà¨Ëô´ÂêØÂä®")
    print("=" * 60)

    imdb_ids = read_imdb_ids_from_file(IMDB_ID_FILE)
    if not imdb_ids:
        print("‚ö†Ô∏è Ê≤°ÊúâÂèØÂ§ÑÁêÜÁöÑIDÔºåÁ®ãÂ∫èÈÄÄÂá∫")
        exit()

    start = time.time()
    failed = fetch_all_parallel(imdb_ids, OUTPUT_DIR)

    print("\nüìä ÊÄªÊï∞: ", len(imdb_ids))
    print("‚úÖ ÊàêÂäü: ", len(imdb_ids) - len(failed))
    print("‚ùå Â§±Ë¥•: ", len(failed))
    print(f"‚è±Ô∏è ÊÄªËÄóÊó∂: {int(time.time() - start)} Áßí")

    if failed:
        with open(FAILED_FILE, "w", encoding="utf-8") as f:
            f.write("\n".join(failed))
        print(f"\nüìÅ Â§±Ë¥•IDÂ∑≤ÂÜôÂÖ•: {FAILED_FILE}")

    input("\nüéâ ÂÆåÊàêÔºÅÊåâEnterÈÄÄÂá∫...")
