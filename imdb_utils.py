#!/usr/bin/env python
# -*- coding: UTF-8 -*-

"""
@Project    ï¼šIMDBCrawler
@File       ï¼šimdb_utils.py
@Author     ï¼šIronmanJay
@Date       ï¼š2025/7/5 14:02
@Describe   ï¼šä¸€äº›è¾…åŠ©å·¥å…·
"""

import os
import glob
import zstandard as zstd
import tarfile
from concurrent.futures import ThreadPoolExecutor
import time
import argparse
import io
from tqdm import tqdm
import shutil
import math



class IMDbDataCleaner:
    def __init__(self, data_file="data.txt", html_dir=r"D:\imdb_plots"):
        self.data_file = data_file
        self.html_dir = html_dir

    def load_html_ids(self):
        html_ids = set()
        if not os.path.exists(self.html_dir):
            print(f"âŒ HTMLç›®å½•ä¸å­˜åœ¨: {self.html_dir}")
            return html_ids

        for fname in os.listdir(self.html_dir):
            if fname.lower().endswith(".html") and fname.startswith("tt"):
                html_id = os.path.splitext(fname)[0]
                html_ids.add(html_id)
        print(f"ğŸ“ ä»HTMLç›®å½•ä¸­æå–åˆ° {len(html_ids)} ä¸ªID")
        return html_ids

    def clean_data_file(self):
        if not os.path.exists(self.data_file):
            print(f"âŒ data.txt æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            return

        with open(self.data_file, "r", encoding="utf-8") as f:
            original_ids = [line.strip() for line in f if line.strip()]
        print(f"ğŸ“„ åŸå§‹data.txtä¸­å…±æœ‰ {len(original_ids)} ä¸ªID")

        html_ids = self.load_html_ids()

        remaining_ids = [id_ for id_ in original_ids if id_ not in html_ids]

        with open(self.data_file, "w", encoding="utf-8") as f:
            f.write("\n".join(remaining_ids) + ("\n" if remaining_ids else ""))
        print(f"âœ… æ¸…æ´—å®Œæˆï¼Œå‰©ä½™ {len(remaining_ids)} ä¸ªIDå†™å› data.txt")

    def run(self):
        print("=" * 60)
        print("ğŸ”§ IMDb ID æ¸…æ´—å·¥å…·å¯åŠ¨")
        print("=" * 60)
        self.clean_data_file()
        print("ğŸ‰ æ¸…æ´—ä»»åŠ¡å®Œæˆ")


class IMDbDataSplitter:
    def __init__(self, data_file="data.txt", output_dir="."):
        self.data_file = data_file
        self.output_dir = output_dir

    def split_data_file(self):
        if not os.path.exists(self.data_file):
            print(f"âŒ data.txt æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            return

        with open(self.data_file, "r", encoding="utf-8") as f:
            ids = [line.strip() for line in f if line.strip()]

        total = len(ids)
        if total == 0:
            print("âš ï¸ data.txt ä¸ºç©ºï¼Œæ— æ³•åˆ†å‰²")
            return

        mid = total // 2
        part1 = ids[:mid]
        part2 = ids[mid:]

        part1_path = os.path.join(self.output_dir, "data_part1.txt")
        part2_path = os.path.join(self.output_dir, "data_part2.txt")

        with open(part1_path, "w", encoding="utf-8") as f1:
            f1.write("\n".join(part1) + "\n")
        with open(part2_path, "w", encoding="utf-8") as f2:
            f2.write("\n".join(part2) + "\n")

        print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…± {total} ä¸ªIDï¼š")
        print(f"    ğŸ“„ å†™å…¥ {len(part1)} è¡Œåˆ° {part1_path}")
        print(f"    ğŸ“„ å†™å…¥ {len(part2)} è¡Œåˆ° {part2_path}")

    def run(self):
        print("=" * 60)
        print("âœ‚ï¸ IMDb ID åˆ†å‰²å·¥å…·å¯åŠ¨")
        print("=" * 60)
        self.split_data_file()
        print("ğŸ‰ åˆ†å‰²ä»»åŠ¡å®Œæˆ")


class HTMLArchiveCompressor:
    def __init__(self, directory, output_file, compression_level=3, max_workers=None,
                 keep_original=False, verbose=False):
        """
        åˆå§‹åŒ–HTMLå½’æ¡£å‹ç¼©å™¨

        å‚æ•°:
            directory: åŒ…å«HTMLæ–‡ä»¶çš„ç›®å½•è·¯å¾„
            output_file: è¾“å‡ºçš„å‹ç¼©æ–‡ä»¶è·¯å¾„
            compression_level: Zstdå‹ç¼©çº§åˆ«(1-22, é»˜è®¤3)
            max_workers: çº¿ç¨‹æ± å¤§å°(é»˜è®¤=CPUæ ¸å¿ƒæ•°Ã—2)
            keep_original: æ˜¯å¦ä¿ç•™åŸå§‹HTMLæ–‡ä»¶(é»˜è®¤False)
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º(é»˜è®¤False)
        """
        self.directory = directory
        self.output_file = output_file
        self.compression_level = compression_level
        self.keep_original = keep_original
        self.verbose = verbose

        # è‡ªåŠ¨ç¡®å®šåˆé€‚çš„çº¿ç¨‹æ•°
        self.max_workers = max_workers or (os.cpu_count() * 2 if os.cpu_count() else 8)

        # æ”¶é›†HTMLæ–‡ä»¶
        self.html_files = self._find_html_files()
        self.total_files = len(self.html_files)
        self.processed_files = 0
        self.start_time = None

    def _find_html_files(self):
        """æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰HTMLæ–‡ä»¶"""
        pattern = os.path.join(self.directory, '**/*.html')
        return glob.glob(pattern, recursive=True)

    def compress(self):
        """å°†æ‰€æœ‰HTMLæ–‡ä»¶å‹ç¼©åˆ°å•ä¸ªå‹ç¼©åŒ…"""
        if not self.html_files:
            print("No HTML files found in the directory!")
            return False

        self.start_time = time.time()
        print(f"Found {self.total_files} HTML files.")
        print(f"Starting compression to {self.output_file}...")
        print(f"Compression level: {self.compression_level}")
        print(f"Original files will {'be kept' if self.keep_original else 'be deleted'}")

        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            os.makedirs(os.path.dirname(self.output_file), exist_ok=True)

            # ä½¿ç”¨Zstdå‹ç¼©å™¨
            cctx = zstd.ZstdCompressor(level=self.compression_level, threads=0)

            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = tqdm(total=self.total_files, disable=not self.verbose,
                                desc="Compressing files", unit="file")

            # æ‰“å¼€è¾“å‡ºæ–‡ä»¶
            with open(self.output_file, 'wb') as f_out:
                # åˆ›å»ºZstdå‹ç¼©æµ
                with cctx.stream_writer(f_out) as compressor:
                    # åˆ›å»ºtarå½’æ¡£å™¨
                    with tarfile.open(fileobj=compressor, mode='w|') as tar:
                        # æ·»åŠ æ‰€æœ‰HTMLæ–‡ä»¶åˆ°tarå½’æ¡£
                        for file_path in self.html_files:
                            # è®¡ç®—åœ¨å½’æ¡£ä¸­çš„ç›¸å¯¹è·¯å¾„
                            arcname = os.path.relpath(file_path, self.directory)

                            # æ·»åŠ æ–‡ä»¶åˆ°tar
                            tar.add(file_path, arcname=arcname)

                            # æ›´æ–°è¿›åº¦
                            self.processed_files += 1
                            progress_bar.update(1)

                            # åˆ é™¤åŸå§‹æ–‡ä»¶ï¼ˆå¦‚æœè®¾ç½®ï¼‰
                            if not self.keep_original:
                                os.remove(file_path)

            progress_bar.close()

            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            elapsed_time = time.time() - self.start_time
            print(f"\nCompression completed!")
            print(f"Successfully compressed {self.total_files} files into {self.output_file}")
            print(f"Total time: {elapsed_time:.2f} seconds")
            print(f"Average speed: {self.total_files / elapsed_time:.2f} files/sec")

            return True
        except Exception as e:
            print(f"Error during compression: {str(e)}")
            return False


class ZstdTarExtractor:
    def __init__(self, archive_path, extract_dir, verbose=False):
        """
        åˆå§‹åŒ–Zstd Tarè§£å‹å™¨

        å‚æ•°:
            archive_path: .tar.zstå‹ç¼©æ–‡ä»¶è·¯å¾„
            extract_dir: è§£å‹ç›®æ ‡ç›®å½•
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        """
        self.archive_path = archive_path
        self.extract_dir = extract_dir
        self.verbose = verbose

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(archive_path):
            raise FileNotFoundError(f"Archive file not found: {archive_path}")

    def extract(self):
        """è§£å‹.tar.zstæ–‡ä»¶"""
        # åˆ›å»ºè§£å‹ç›®å½•
        os.makedirs(self.extract_dir, exist_ok=True)

        print(f"Extracting {os.path.basename(self.archive_path)} to {self.extract_dir}")

        # è·å–å‹ç¼©æ–‡ä»¶å¤§å°ç”¨äºè¿›åº¦æ¡
        total_size = os.path.getsize(self.archive_path)

        try:
            # ä½¿ç”¨Zstdè§£å‹å™¨
            dctx = zstd.ZstdDecompressor()

            # æ‰“å¼€å‹ç¼©æ–‡ä»¶
            with open(self.archive_path, 'rb') as f_in:
                # åˆ›å»ºè§£å‹æµ
                with dctx.stream_reader(f_in) as decompressed:
                    # ä½¿ç”¨tarfileæ‰“å¼€è§£å‹åçš„æ•°æ®æµ
                    with tarfile.open(fileobj=decompressed, mode='r|') as tar:
                        # åˆ›å»ºè¿›åº¦æ¡
                        pbar = tqdm(desc="Extracting", total=total_size,
                                    unit='B', unit_scale=True, disable=not self.verbose)

                        # æå–æ‰€æœ‰æ–‡ä»¶
                        for member in tar:
                            # æ˜¾ç¤ºå½“å‰æå–çš„æ–‡ä»¶ï¼ˆå¦‚æœverboseï¼‰
                            if self.verbose:
                                print(f"Extracting: {member.name}")

                            tar.extract(member, self.extract_dir)

                            # æ›´æ–°è¿›åº¦æ¡ï¼ˆåŸºäºå·²å¤„ç†çš„æ•°æ®é‡ï¼‰
                            pbar.update(f_in.tell() - pbar.n)

                        pbar.close()

            print(f"Successfully extracted to {self.extract_dir}")
            return True

        except Exception as e:
            print(f"Extraction failed: {str(e)}")
            return False


class FastHTMLBatchSplitter:
    def __init__(self, source_dir, batch_size=20000, target_dir=None, workers=8):
        """
        è¶…é«˜é€ŸHTMLæ–‡ä»¶åˆ†æ‰¹å™¨

        å‚æ•°:
            source_dir: æºç›®å½•è·¯å¾„
            batch_size: æ¯æ‰¹æ–‡ä»¶æ•° (é»˜è®¤20,000)
            target_dir: ç›®æ ‡ç›®å½• (é»˜è®¤: source_dir/batches)
            workers: å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤8)
        """
        self.source_dir = os.path.normpath(source_dir)
        self.batch_size = batch_size
        self.target_root = target_dir or os.path.join(source_dir, "batches")
        self.workers = workers
        self.file_chunks = []
        self.processed_files = 0  # æ–°å¢ï¼šè·Ÿè¸ªå·²å¤„ç†æ–‡ä»¶æ•°

    def _find_html_files(self):
        """å¤šçº¿ç¨‹å¿«é€Ÿæ‰«æHTMLæ–‡ä»¶"""
        print("ğŸš€ æ­£åœ¨åŠ é€Ÿæ‰«æHTMLæ–‡ä»¶...")
        all_files = []

        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            # å¹¶è¡Œéå†ç›®å½•
            for root, _, files in os.walk(self.source_dir):
                if os.path.normpath(root).startswith(os.path.normpath(self.target_root)):
                    continue
                all_files.extend(os.path.join(root, f) for f in files if f.lower().endswith('.html'))

        return all_files

    def _prepare_batches(self):
        """é¢„åˆ†é…æ–‡ä»¶åˆ°æ‰¹æ¬¡"""
        total_files = len(self.html_files)
        num_batches = math.ceil(total_files / self.batch_size)

        # é¢„åˆ‡å‰²æ–‡ä»¶åˆ—è¡¨é¿å…åŠ¨æ€åˆ†é…
        self.file_chunks = [
            self.html_files[i * self.batch_size: (i + 1) * self.batch_size]
            for i in range(num_batches)
        ]

    def _process_batch(self, batch_num, files, pbar):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ (çº¿ç¨‹å®‰å…¨)"""
        batch_dir = os.path.join(self.target_root, f"batch_{batch_num + 1:03d}")

        for src_file in files:
            try:
                rel_path = os.path.relpath(src_file, self.source_dir)
                dst_file = os.path.join(batch_dir, rel_path)
                os.makedirs(os.path.dirname(dst_file), exist_ok=True)
                shutil.move(src_file, dst_file)
                if pbar:  # æ–°å¢ï¼šå®‰å…¨æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
            except Exception as e:
                print(f"\nâš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥: {src_file} - {str(e)}")

    def split_into_batches(self):
        """å¤šçº¿ç¨‹åˆ†æ‰¹å¤„ç†"""
        self.html_files = self._find_html_files()

        if not self.html_files:
            print("âŒ æœªæ‰¾åˆ°HTMLæ–‡ä»¶ï¼")
            return False

        print(f"ğŸ“Š å…±æ‰¾åˆ° {len(self.html_files):,} ä¸ªHTMLæ–‡ä»¶")
        self._prepare_batches()

        # åˆ›å»ºç›®æ ‡ç›®å½•
        os.makedirs(self.target_root, exist_ok=True)

        # åˆå§‹åŒ–è¿›åº¦æ¡
        with tqdm(total=len(self.html_files), desc="âš¡ åŠ é€Ÿå¤„ç†", unit="æ–‡ä»¶") as pbar:
            with ThreadPoolExecutor(max_workers=self.workers) as executor:
                futures = []
                for i, chunk in enumerate(self.file_chunks):
                    future = executor.submit(self._process_batch, i, chunk, pbar)
                    futures.append(future)

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for future in futures:
                    future.result()

        # å®‰å…¨è·å–å¤„ç†é€Ÿåº¦
        speed = len(self.html_files) / (pbar.format_dict['elapsed'] or 1)
        print(f"\nâœ… å¤„ç†å®Œæˆï¼å¹³å‡é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/ç§’")
        return True


if __name__ == "__main__":
    state = "splithtml"

    if state == "clean":
        # Step 1: æ¸…æ´— data.txt
        cleaner = IMDbDataCleaner(
            data_file="data.txt",
            html_dir=r"D:\imdb_plots"
        )
        cleaner.run()
    elif state == "split":
        # Step 2: åˆ†å‰² data.txt
        splitter = IMDbDataSplitter(
            data_file="data.txt",
            output_dir="."  # å½“å‰ç›®å½•
        )
        splitter.run()
    elif state == "compressor":
        # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
        parser = argparse.ArgumentParser(description='HTMLæ–‡ä»¶å½’æ¡£å‹ç¼©å·¥å…·')
        parser.add_argument('directory', type=str, help='åŒ…å«HTMLæ–‡ä»¶çš„ç›®å½•è·¯å¾„')
        parser.add_argument('output', type=str, help='è¾“å‡ºå‹ç¼©æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--level', type=int, default=3,
                            help='å‹ç¼©çº§åˆ«(1-22, é»˜è®¤3)')
        parser.add_argument('--workers', type=int, default=None,
                            help='çº¿ç¨‹æ•°(é»˜è®¤=è‡ªåŠ¨æ£€æµ‹)')
        parser.add_argument('--keep', action='store_true',
                            help='ä¿ç•™åŸå§‹æ–‡ä»¶(é»˜è®¤åˆ é™¤)')
        parser.add_argument('--verbose', action='store_true',
                            help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')

        args = parser.parse_args()

        # åˆ›å»ºå‹ç¼©å™¨å®ä¾‹å¹¶æ‰§è¡Œå‹ç¼©
        compressor = HTMLArchiveCompressor(
            directory=args.directory,
            output_file=args.output,
            compression_level=args.level,
            max_workers=args.workers,
            keep_original=args.keep,
            verbose=args.verbose
        )

        success = compressor.compress()

        if success:
            print("All files compressed successfully into a single archive!")
        else:
            print("Compression failed.")
    if state == "decompressor":
        # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
        parser = argparse.ArgumentParser(description='Zstd Tarè§£å‹å·¥å…·')
        parser.add_argument('archive', type=str, help='.tar.zstæ–‡ä»¶è·¯å¾„')
        parser.add_argument('output_dir', type=str, help='è§£å‹ç›®æ ‡ç›®å½•')
        parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')

        args = parser.parse_args()

        # åˆ›å»ºè§£å‹å™¨å®ä¾‹å¹¶æ‰§è¡Œè§£å‹
        extractor = ZstdTarExtractor(
            archive_path=args.archive,
            extract_dir=args.output_dir,
            verbose=args.verbose
        )

        success = extractor.extract()

        if success:
            print("è§£å‹æˆåŠŸï¼")
        else:
            print("è§£å‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    elif state=="splithtml":
        parser = argparse.ArgumentParser(description='è¶…é«˜é€ŸHTMLæ–‡ä»¶åˆ†æ‰¹å™¨')
        parser.add_argument('source_dir', help='æºç›®å½•è·¯å¾„')
        parser.add_argument('--batch-size', type=int, default=20000, help='æ¯æ‰¹æ–‡ä»¶æ•°')
        parser.add_argument('--target-dir', help='è‡ªå®šä¹‰ç›®æ ‡ç›®å½•')
        parser.add_argument('--workers', type=int, default=8, help='å¹¶è¡Œçº¿ç¨‹æ•°')

        args = parser.parse_args()

        print(f"âš¡ å¯åŠ¨è¶…é«˜é€Ÿå¤„ç† (çº¿ç¨‹æ•°: {args.workers})")
        splitter = FastHTMLBatchSplitter(
            source_dir=args.source_dir,
            batch_size=args.batch_size,
            target_dir=args.target_dir,
            workers=args.workers
        )

        splitter.split_into_batches()
